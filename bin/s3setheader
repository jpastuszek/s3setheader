#!/usr/bin/env ruby
require 'cli'
require 'right_aws'
require 'logger'
require_relative '../lib/s3setheader'

settings = CLI.new do
	description 'Set header of S3 object'

	option :key_id,
		short: :i,
		description: 'AWS access key ID',
		required: true
	option :key_secret,
		short: :s,
		description: 'AWS access key secret',
		required: true

	option :bucket,
		short: :b,
		description: 'bucket to precess',
		required: true
		
	options :header,
		short: :H,
		description: '<header>=<value> to set',
		cast: lambda {|v| v.split('=', 2)}

	option :lister_fetch_size,
		description: 'fetch no more that that number of keys per request',
		cast: Integer,
		default: 200

	option :lister_backlog,
		description: 'maximum length of to be processed key queue',
		cast: Integer,
		default: 1000

	option :reporter_backlog,
		description: 'maximum length of to be processed key queue',
		cast: Integer,
		default: 1000
	option :reporter_summary_interval,
		description: 'pring summary every some number of processed objects',
		cast: Integer,
		default: 100
	option :reporter_average_contribution,
		description: 'how much does last average calculation contribute in the printed value - less => more stable',
		cast: Float,
		default: 0.10

	option :workers,
		short: :t,
		description: 'number of processing threads to start',
		cast: Integer,
		default: 10

	switch :debug,
		short: :d,
		description: 'log at DEBUG level'

	argument :prefix,
		description: 'process only object of key starting with given prefix',
		required: false
end.parse!

log = Logger.new(STDERR)
log.level = settings.debug ? Logger::DEBUG : Logger::INFO

log.debug(settings.inspect)

generated_header = ["x-amz-id-2", "x-amz-request-id", "date", "last-modified", "etag", "accept-ranges", "content-length", "server"]
set_headers = Hash[settings.header.map{|k, v| [k.downcase, v]}]

log.info "Setting headers: #{set_headers}"

BucketProcessor.new(settings.key_id, settings.key_secret, settings.bucket, 
		log: log,
		workers: settings.workers,
		lister_fetch_size: settings.lister_fetch_size,
		lister_backlog: settings.lister_backlog,
		reporter_backlog: settings.reporter_backlog,
		reporter_summary_interval: settings.reporter_summary_interval,
		reporter_average_contribution: settings.reporter_average_contribution
	) do |bucket, key|
		key.head
		headers_to_copy = key.headers.keys - generated_header

		new_headers = key.headers.select{|header| headers_to_copy.include? header}
		log.debug{"#{key} - original headers: #{new_headers}"}

		new_headers.merge!(set_headers)
		log.debug{"#{key} -      new headers: #{new_headers}"}

		# Main HACK over S3 is to use :replace copy on the same object to avoid data copy
		bucket.s3.interface.copy(bucket.name, key.name, bucket.name, key.name, :replace, new_headers)
end
.run(settings.prefix)

