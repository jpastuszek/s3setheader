#!/usr/bin/env ruby

require 'cli'
require 'right_aws'
require 'logger'
require 'thread'
require_relative '../lib/s3setheader'

settings = CLI.new do
	description 'Set header of S3 object'

	option :key_id,
		short: :i,
		description: 'AWS access key ID',
		required: true
	option :key_secret,
		short: :s,
		description: 'AWS access key secret',
		required: true

	option :bucket,
		short: :b,
		description: 'bucket to precess',
		required: true
		
	options :header,
		short: :H,
		description: '<header>=<value> to set',
		cast: lambda {|v| v.split('=', 2)}

	option :lister_fetch_size,
		description: 'fetch no more that that number of keys per request',
		cast: Integer,
		default: 200

	option :lister_backlog,
		description: 'maximum length of to be processed key queue',
		cast: Integer,
		default: 1000

	option :reporter_backlog,
		description: 'maximum length of to be processed key queue',
		cast: Integer,
		default: 1000

	option :workers,
		short: :t,
		description: 'number of processing threads to start',
		cast: Integer,
		default: 10

	switch :debug,
		short: :d,
		description: 'log at DEBUG level'
end.parse!

log = Logger.new(STDERR)
log.level = settings.debug ? Logger::DEBUG : Logger::INFO

Thread.abort_on_exception = true

log.debug(settings.inspect)

s3 = RightAws::S3.new(settings.key_id, settings.key_secret, multi_thread: true, logger: log)
bucket = s3.bucket(settings.bucket)

keys = SizedQueue.new(settings.lister_backlog)

begin
	reporter = Reporter.new(settings.reporter_backlog) do |reports|
		total_listed_keys = 0
		total_processed_keys = 0
		total_succeeded_keys = 0
		total_failed_keys = 0

		processed_avg = 0.0
		last_time = nil
		last_total = 0

		reports.each do |key, value|
			case key
			when :new_keys_count
				total_listed_keys += value
			when :processed_key
				total_processed_keys += 1
				if total_processed_keys % 10 == 0
					if last_time
						contribution = 0.25
						new = (total_processed_keys - last_total).to_f / (Time.now.to_f - last_time)
						processed_avg = processed_avg * (1.0 - contribution) + new * contribution
					end
					last_time = Time.now.to_f
					last_total = total_processed_keys

					log.info "-- %6d: failed: %d (%.2f %%) @ %.1f/s" % [
						total_processed_keys,
						total_failed_keys,
						total_failed_keys.to_f / total_processed_keys * 100,
						processed_avg
					]
				end
			when :succeeded_key
				total_succeeded_keys += 1
			when :failed_key
				key, error = *value
				log.error "Key processing failed: #{key}: #{error.class.name}, #{error.message}"
				total_failed_keys += 1
			end
			log.debug("Report: #{key}: #{value}")
		end

		reports.on_finish do
			log.info("Total listed keys: #{total_listed_keys}")
			log.info("Total processed keys: #{total_processed_keys}")
			log.info("Total succeeded keys: #{total_succeeded_keys}")
			log.info("Total failed keys: #{total_failed_keys}")
		end
	end
	.run

	Lister.new(bucket, keys, settings.lister_fetch_size)
	.on_keys_chunk do |keys_chunk|
		log.debug "Got #{keys_chunk.length} new keys"
		reporter.report(:new_keys_count, keys_chunk.length)
	end
	.on_finish do
		log.info "Done listing keys"
		# notify all workers that no more messages will be posted
		settings.workers.times{ keys << :end }
	end
	.run

	# create workers
	log.info "Lounching #{settings.workers} workers"
	workers = (1..settings.workers).to_a.map do |worker_no|
		Worker.new(worker_no, keys) do |key|
			log.debug "Worker[#{worker_no}]: Processing key #{key}"
			reporter.report :processed_key, key
			reporter.report :succeeded_key, key
		end
		.on_error do |key, error|
			reporter.report :processed_key, key
			reporter.report :failed_key, [key, error]
		end
		.on_finish do
			log.info "Worker #{worker_no} done"
		end
		.run
	end

	# wait for all to finish
	workers.each(&:join)
	log.info "All workers done"
	reporter.join
rescue Interrupt
	# flush thread waiting on queues
	keys.max = 999999 
end

